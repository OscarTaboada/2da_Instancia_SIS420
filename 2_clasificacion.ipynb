{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 388,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "# Modulo de optimización\n",
        "from scipy import optimize\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOV46UUj1Zgm"
      },
      "source": [
        "## Cargar datos\n",
        "### Dataset\n",
        "Atributos del dataset:\n",
        "1. Marca\n",
        "2. Modelo (Ej: 2021, 2016)\n",
        "3. Número de puertas\n",
        "4. Número de asientos\n",
        "5. Condición del vehículo\n",
        "6. Tipo\n",
        "8. Transmisión\n",
        "9. Tipo de Combustible\n",
        "7. Kilometraje (recorrido en Km)\n",
        "10. Color\n",
        "11. Procedencia\n",
        "12. Cilindrada (en cc)\n",
        "13. Estado de la pintura\n",
        "14. ¿La pintura está en buen estado?\n",
        "15. ¿El vehículo tuvo algún accidente?\n",
        "16. ¿El vehículo cuenta con todos los accesorios?\n",
        "17. Costo en $us\n",
        "18. Tipo de costo (caro=1; algo caro=2; accesible=3; muy accesible=4; barato=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {
        "id": "MXB1YI3xy-KH"
      },
      "outputs": [],
      "source": [
        "data_train = np.loadtxt('DVO.csv', delimiter=',', dtype=float)\n",
        "data_test = np.loadtxt('prueba.csv', delimiter=',', dtype=float)\n",
        "data_test2 = np.loadtxt('prueba2.csv', delimiter=',', dtype=float)\n",
        "#print(data_train)\n",
        "# print(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### X: Marca, año de fabricación, Cilindrada, Kilometraje, Tipo de Movilidad y estado interior y exterior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 440,
      "metadata": {
        "id": "Z_cGbs2h3jyp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.20000e+01 2.01700e+03 3.00000e+00 ... 4.23970e+04 1.20000e+03\n",
            "  2.00000e+00]\n",
            " [1.90000e+01 2.01900e+03 2.00000e+00 ... 0.00000e+00 3.70000e+03\n",
            "  2.00000e+00]\n",
            " [2.00000e+00 2.01800e+03 2.00000e+00 ... 3.80000e+04 2.70000e+03\n",
            "  2.00000e+00]\n",
            " ...\n",
            " [1.90000e+01 2.00300e+03 3.00000e+00 ... 6.40000e+04 1.60000e+03\n",
            "  1.00000e+00]\n",
            " [3.50000e+01 1.98000e+03 4.00000e+00 ... 2.00000e+05 1.50000e+03\n",
            "  1.00000e+00]\n",
            " [1.70000e+01 1.99200e+03 3.00000e+00 ... 2.65918e+05 2.50000e+03\n",
            "  1.00000e+00]]\n"
          ]
        }
      ],
      "source": [
        "# X = data_train[:,0:17]\n",
        "X = data_train[:,[0, 1, 4, 5, 8, 11, 15]]\n",
        "# X_test = data_test[:,0:17]\n",
        "X_test = data_test[:,[0, 1, 4, 5, 8, 11, 15]]\n",
        "X_test2 = data_test2[:,[0, 1, 4, 5, 8, 11, 15]]\n",
        "print(X)\n",
        "#print(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 441,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N98edbfZ1vRv",
        "outputId": "57fdecc5-87bc-4a8e-e421-021e3b185fd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((800, 7), (800,))"
            ]
          },
          "execution_count": 441,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = data_train[:,17].astype(int)\n",
        "# y = np.asarray(y, dtype=np.int)\n",
        "y_test = data_test[:,17].astype(int)\n",
        "# y_test = np.asarray(y_test, dtype=np.int)\n",
        "y_test2 = data_test2[:,17].astype(int)\n",
        "X.shape, y.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 442,
      "metadata": {
        "id": "16K1o3MGdAL9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X \n",
            " [[1.20000e+01 2.01700e+03 3.00000e+00 ... 4.23970e+04 1.20000e+03\n",
            "  2.00000e+00]\n",
            " [1.90000e+01 2.01900e+03 2.00000e+00 ... 0.00000e+00 3.70000e+03\n",
            "  2.00000e+00]\n",
            " [2.00000e+00 2.01800e+03 2.00000e+00 ... 3.80000e+04 2.70000e+03\n",
            "  2.00000e+00]\n",
            " ...\n",
            " [1.90000e+01 2.00300e+03 3.00000e+00 ... 6.40000e+04 1.60000e+03\n",
            "  1.00000e+00]\n",
            " [3.50000e+01 1.98000e+03 4.00000e+00 ... 2.00000e+05 1.50000e+03\n",
            "  1.00000e+00]\n",
            " [1.70000e+01 1.99200e+03 3.00000e+00 ... 2.65918e+05 2.50000e+03\n",
            "  1.00000e+00]] \n",
            " Y\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
            " 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
            " 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5]\n"
          ]
        }
      ],
      "source": [
        "print('X \\n',X,'\\n Y\\n',y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funcion activacion relu\n",
        "La función relu transforma los valores introducidos anulando los valores negativos y dejando los positivos tal y como entran"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 443,
      "metadata": {
        "id": "GjnTX8mo_l7X"
      },
      "outputs": [],
      "source": [
        "#funciondes de activacion\n",
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### En cuanto a las funciones de activación que utilizaremos a la salida del MLP\n",
        "\n",
        "Lineal: usada para regresión (junto a la función de pérdida MSE).\n",
        "\n",
        "Sigmoid: usada para clasificación binaria (junto a la función de pérdida BCE).\n",
        "\n",
        "Softmax: usada para clasificación multiclase (junto a la función de pérdida crossentropy, CE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 444,
      "metadata": {
        "id": "V_1-jsm8_m4H"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J55vV7OcET20"
      },
      "source": [
        "### funciones de perdida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 445,
      "metadata": {
        "id": "oElbryEBBO-Z"
      },
      "outputs": [],
      "source": [
        "# Mean Square Error -> usada para regresión (con activación lineal)\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
        "def bce(y, y_hat):\n",
        "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
        "\n",
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZugCMy9EXSS"
      },
      "source": [
        "Derivadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 446,
      "metadata": {
        "id": "i9fIvOu3rn52"
      },
      "outputs": [],
      "source": [
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_bce(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_crossentropy(y, y_hat):\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    answers[np.arange(len(y_hat)),y] = 1    \n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 447,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:54.342062Z",
          "start_time": "2020-08-05T16:49:54.325063Z"
        },
        "code_folding": [
          3,
          14,
          20,
          55
        ],
        "id": "7O__X_wZA7qf"
      },
      "outputs": [],
      "source": [
        "# clase base MLP\n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2\n",
        "    return self.activation(y_hat)\n",
        "\n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):\n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size]\n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x)\n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)\n",
        "            # Backprop\n",
        "            dldy = self.grad_loss(y, y_pred)\n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)\n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 448,
      "metadata": {
        "id": "UjMkaUfLGiVO"
      },
      "outputs": [],
      "source": [
        "# MLP para regresión\n",
        "class MLPRegression(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
        "\n",
        "# MLP para clasificación binaria\n",
        "class MLPBinaryClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
        "\n",
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, linear)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 449,
      "metadata": {
        "id": "hw83l4cKUtet"
      },
      "outputs": [],
      "source": [
        "# normalización datos\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRc_nGtSMOUd",
        "outputId": "372eea07-bced-455d-f8d3-9f951e46f9b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10/1000, Loss: 1.62106\n",
            "Epoch: 20/1000, Loss: 1.52184\n",
            "Epoch: 30/1000, Loss: 1.45382\n",
            "Epoch: 40/1000, Loss: 1.40388\n",
            "Epoch: 50/1000, Loss: 1.36523\n",
            "Epoch: 60/1000, Loss: 1.33406\n",
            "Epoch: 70/1000, Loss: 1.30808\n",
            "Epoch: 80/1000, Loss: 1.28589\n",
            "Epoch: 90/1000, Loss: 1.26658\n",
            "Epoch: 100/1000, Loss: 1.24953\n",
            "Epoch: 110/1000, Loss: 1.23428\n",
            "Epoch: 120/1000, Loss: 1.22052\n",
            "Epoch: 130/1000, Loss: 1.20799\n",
            "Epoch: 140/1000, Loss: 1.19650\n",
            "Epoch: 150/1000, Loss: 1.18592\n",
            "Epoch: 160/1000, Loss: 1.17610\n",
            "Epoch: 170/1000, Loss: 1.16698\n",
            "Epoch: 180/1000, Loss: 1.15846\n",
            "Epoch: 190/1000, Loss: 1.15049\n",
            "Epoch: 200/1000, Loss: 1.14301\n",
            "Epoch: 210/1000, Loss: 1.13598\n",
            "Epoch: 220/1000, Loss: 1.12935\n",
            "Epoch: 230/1000, Loss: 1.12309\n",
            "Epoch: 240/1000, Loss: 1.11717\n",
            "Epoch: 250/1000, Loss: 1.11155\n",
            "Epoch: 260/1000, Loss: 1.10622\n",
            "Epoch: 270/1000, Loss: 1.10114\n",
            "Epoch: 280/1000, Loss: 1.09629\n",
            "Epoch: 290/1000, Loss: 1.09167\n",
            "Epoch: 300/1000, Loss: 1.08725\n",
            "Epoch: 310/1000, Loss: 1.08302\n",
            "Epoch: 320/1000, Loss: 1.07897\n",
            "Epoch: 330/1000, Loss: 1.07507\n",
            "Epoch: 340/1000, Loss: 1.07133\n",
            "Epoch: 350/1000, Loss: 1.06774\n",
            "Epoch: 360/1000, Loss: 1.06428\n",
            "Epoch: 370/1000, Loss: 1.06094\n",
            "Epoch: 380/1000, Loss: 1.05772\n",
            "Epoch: 390/1000, Loss: 1.05462\n",
            "Epoch: 400/1000, Loss: 1.05162\n",
            "Epoch: 410/1000, Loss: 1.04871\n",
            "Epoch: 420/1000, Loss: 1.04590\n",
            "Epoch: 430/1000, Loss: 1.04318\n",
            "Epoch: 440/1000, Loss: 1.04054\n",
            "Epoch: 450/1000, Loss: 1.03798\n",
            "Epoch: 460/1000, Loss: 1.03549\n",
            "Epoch: 470/1000, Loss: 1.03307\n",
            "Epoch: 480/1000, Loss: 1.03072\n",
            "Epoch: 490/1000, Loss: 1.02844\n",
            "Epoch: 500/1000, Loss: 1.02622\n",
            "Epoch: 510/1000, Loss: 1.02405\n",
            "Epoch: 520/1000, Loss: 1.02194\n",
            "Epoch: 530/1000, Loss: 1.01989\n",
            "Epoch: 540/1000, Loss: 1.01788\n",
            "Epoch: 550/1000, Loss: 1.01592\n",
            "Epoch: 560/1000, Loss: 1.01401\n",
            "Epoch: 570/1000, Loss: 1.01214\n",
            "Epoch: 580/1000, Loss: 1.01031\n",
            "Epoch: 590/1000, Loss: 1.00852\n",
            "Epoch: 600/1000, Loss: 1.00677\n",
            "Epoch: 610/1000, Loss: 1.00505\n",
            "Epoch: 620/1000, Loss: 1.00337\n",
            "Epoch: 630/1000, Loss: 1.00172\n",
            "Epoch: 640/1000, Loss: 1.00011\n",
            "Epoch: 650/1000, Loss: 0.99852\n",
            "Epoch: 660/1000, Loss: 0.99697\n",
            "Epoch: 670/1000, Loss: 0.99544\n",
            "Epoch: 680/1000, Loss: 0.99395\n",
            "Epoch: 690/1000, Loss: 0.99248\n",
            "Epoch: 700/1000, Loss: 0.99103\n",
            "Epoch: 710/1000, Loss: 0.98962\n",
            "Epoch: 720/1000, Loss: 0.98822\n",
            "Epoch: 730/1000, Loss: 0.98685\n",
            "Epoch: 740/1000, Loss: 0.98550\n",
            "Epoch: 750/1000, Loss: 0.98418\n",
            "Epoch: 760/1000, Loss: 0.98288\n",
            "Epoch: 770/1000, Loss: 0.98159\n",
            "Epoch: 780/1000, Loss: 0.98033\n",
            "Epoch: 790/1000, Loss: 0.97908\n",
            "Epoch: 800/1000, Loss: 0.97786\n",
            "Epoch: 810/1000, Loss: 0.97665\n",
            "Epoch: 820/1000, Loss: 0.97546\n",
            "Epoch: 830/1000, Loss: 0.97429\n",
            "Epoch: 840/1000, Loss: 0.97313\n",
            "Epoch: 850/1000, Loss: 0.97200\n",
            "Epoch: 860/1000, Loss: 0.97087\n",
            "Epoch: 870/1000, Loss: 0.96976\n",
            "Epoch: 880/1000, Loss: 0.96867\n",
            "Epoch: 890/1000, Loss: 0.96759\n",
            "Epoch: 900/1000, Loss: 0.96653\n",
            "Epoch: 910/1000, Loss: 0.96548\n",
            "Epoch: 920/1000, Loss: 0.96445\n",
            "Epoch: 930/1000, Loss: 0.96343\n",
            "Epoch: 940/1000, Loss: 0.96242\n",
            "Epoch: 950/1000, Loss: 0.96142\n",
            "Epoch: 960/1000, Loss: 0.96043\n",
            "Epoch: 970/1000, Loss: 0.95946\n",
            "Epoch: 980/1000, Loss: 0.95850\n",
            "Epoch: 990/1000, Loss: 0.95755\n",
            "Epoch: 1000/1000, Loss: 0.95661\n"
          ]
        }
      ],
      "source": [
        "model = MLPClassification(D_in=7, H=100, D_out=6)\n",
        "epochs = 1000\n",
        "lr = 0.02\n",
        "model.fit(X_norm, y, epochs, lr, batch_size=100, log_each=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "3\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "3\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "3\n",
            "2\n",
            "1\n",
            "1\n",
            "3\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "4\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "4\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "4\n",
            "4\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "4\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "4\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "4\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "1\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "4\n",
            "2\n",
            "2\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "3\n",
            "4\n",
            "3\n",
            "4\n",
            "3\n",
            "4\n",
            "4\n",
            "4\n",
            "3\n",
            "3\n",
            "4\n",
            "4\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "4\n",
            "1\n",
            "4\n",
            "4\n",
            "4\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "3\n",
            "4\n",
            "4\n",
            "3\n",
            "4\n",
            "5\n",
            "4\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "2\n",
            "4\n",
            "4\n",
            "3\n",
            "2\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "4\n",
            "3\n",
            "3\n",
            "4\n",
            "3\n",
            "3\n",
            "4\n",
            "5\n",
            "5\n",
            "4\n",
            "4\n",
            "3\n",
            "5\n",
            "4\n",
            "5\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "y_predict_ = model.predict(model.ws[999], X_norm)\n",
        "y_pred_train=[]\n",
        "for i in y_predict_:\n",
        "    y_pred_train.append(np.argmax(i))\n",
        "    print(np.argmax(i))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "precicion con datos te entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision de entrenamiento: 65.38%\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_pred, y):\n",
        "    return np.sum(y_pred == y) / len(y)\n",
        "\n",
        "print('Precision de entrenamiento: {:.2f}%'.format(accuracy(y_pred_train, y)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG5C0uIjE7LV"
      },
      "source": [
        "### Datos de prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4])"
            ]
          },
          "execution_count": 452,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 453,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUYwdTX0CMy",
        "outputId": "129a2658-ba2d-4711-ddbe-86bf387e4635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "1\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "1\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "3\n",
            "3\n",
            "3\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "2\n",
            "3\n",
            "4\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "3\n",
            "3\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "1\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "2\n",
            "4\n",
            "3\n",
            "3\n",
            "5\n",
            "3\n",
            "4\n",
            "4\n",
            "1\n",
            "4\n",
            "4\n",
            "4\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "4\n",
            "3\n",
            "4\n",
            "4\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "X_norm_test = (X_test - X_mean) / X_std\n",
        "y_predict = model.predict(model.ws[999], X_norm_test)\n",
        "y_predict_test=[]\n",
        "for i in y_predict:\n",
        "    y_predict_test.append(np.argmax(i))\n",
        "    print(np.argmax(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvPvUzhODRs5",
        "outputId": "5406deec-746f-4802-ee99-837e12a7f88c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision de prueba: 58.89%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print('Precision de prueba: {:.2f}%'.format(accuracy(y_predict_test,y_test)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datos de prueba2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1, 1, 3, 3, 2, 2, 5, 5, 4, 4])"
            ]
          },
          "execution_count": 455,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "2\n",
            "3\n",
            "2\n",
            "1\n",
            "4\n",
            "5\n",
            "4\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "X_norm_test2 = (X_test2 - X_mean) / X_std\n",
        "y_predict = model.predict(model.ws[999], X_norm_test2)\n",
        "y_predict_test2=[]\n",
        "for i in y_predict:\n",
        "    y_predict_test2.append(np.argmax(i))\n",
        "    print(np.argmax(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision de prueba: 50.00%\n"
          ]
        }
      ],
      "source": [
        "print('Precision de prueba: {:.2f}%'.format(accuracy(y_predict_test2,y_test2)*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "1_clasificacion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
